<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Day 3 Introduction to the Eye-tracking Method and its Application in Language Research</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white-cus.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
                    <h2>Day 3</h2>
                    <h3>Spoken Language Processing in a Visual Context</h3>
					<p><img src="assets/feedback-day-3.png" width="15%"></p>
                </section>
				<section data-auto-animate data-auto-animate-id="1">
                    <h3>The visual world paradigm</h3>
					<p class="fragment" style="text-align: left;">Participants hear/produce an utterance while looking at a visual display</p>
					<p class="fragment" style="text-align: left;">While their eye movements are recorded.</p>
					<p class="fragment" style="text-align: left;">Allows to study real-time language comprehension/production in natural tasks. </p>
                </section>
				<section data-auto-animate data-auto-animate-id="1">
					<h3>The visual world paradigm</h3>
					<h4>The visual display</h4>
					<p class="fragment"><img src="assets/vwe-display.png" width="70%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="1">
					<h3>The visual world paradigm</h3>
					<h4>The task</h4>
					<p class="fragment" style="text-align: left;">Action-based: "Pick up the..."</p>
					<p class="fragment" style="text-align: left;">Non action-based: "Anne went to Starbucks and bought coffee."</p>
				</section>
				<section data-auto-animate data-auto-animate-id="1">
					<h3>The visual world paradigm</h3>
					<h4>The linking hypothesis</h4>
					<p class="fragment" style="text-align: left;">How to link gaze position with language processing?</p>
					<p class="fragment" style="text-align: left;"><b>Activation of linguistic representation &rarr; probability of attention shifted to the corresponding picture &rarr; probability of fixation</b></p>
				</section>
				<section data-auto-animate data-auto-animate-id="1">
					<h3>The visual world paradigm</h3>
					<h4>The linking hypothesis</h4>
					<p><img src="assets/allopenna.png" width="55%"></p>
				</section>
				<section>
					<h3>Word recognition in the visual world</h3>
					<h4>Parallel activation during word recognition: Allopenna et al. (1998)</h4>
					<p><img src="assets/allopenna1998visual.png" width="35%">   <img src="assets/allopenna.png" width="35%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Cooper (1974)</h4>
					<p class="fragment" style="text-align: left;">Rapid eye movements to pictures that were named in stories.</p>
					<p class="fragment" style="text-align: left;">Visual attention highly correlated with spoken sentence processing.</p>
					<p><img src="assets/cooper-1976-visual.png" width="30%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Effect of the visual context: Tanenhause et al. (1995)</h4>
					<p class="fragment" style="text-align: left;">Significantly more early looks to the empty towel in the one-referent than in the two-referent condition. </p>
					<p class="fragment" style="text-align: left;">Listeners rapidly use visual context to disambiguate linguistic input. </p>
					<p><img src="assets/Tanenhaus1995.png" width="35%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Syntactic ambiguities: Snedeker and Trueswell (2004)</h4>
					<p><img src="assets/snedeker2004visual.jpg" width="35%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Syntactic ambiguities: Snedeker and Trueswell (2004)</h4>
					<p><img src="assets/snedeker2004result1.jpg" width="32%"> <img src="assets/snedeker2004result2.jpg" width="32%"> <img src="assets/snedeker2004result3.jpg" width="32%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h3>Sentence processing during reading</h3>
					<h4>Is sentence processing incremental?</h4>
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h3>Sentence processing during reading</h3>
					<h4>Is sentence processing incremental?</h4>
					<p>Do we interpret a word as soon as it is identified, or do we wait until the end of a sentence/phrase?</p>
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h3>Sentence processing during reading</h3>
					<h4>Is sentence processing incremental?</h4>
					<p style="text-align: left;" class="fragment">Frazier and Rayner (1982)</p>
					<p style="text-align: left;" class="fragment">(3a) Since Jay always jogs a mile this seems like a short distance to him.</p>
					<p style="text-align: left;" class="fragment">(3b) Since Jay always jogs a mile seems like a short distance to him.</p>
					<p style="text-align: left;" class="fragment">The direct object analysis of <em>a mile</em> is correct in (3a) but not in (3b)</p>
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h3>Sentence processing during reading</h3>
					<h4>Is sentence processing incremental?</h4>
					<p style="text-align: left;">Frazier and Rayner (1982)</p>
					<p style="text-align: left;" class="fragment">Results: Upon encountering <em>seems</em> in (3b) compared with (3a), readers were more likely to make a regressive saccade.</p>
					<p style="text-align: left;" class="fragment">At <em>seems</em>, the reader must already committed to the direct object analysis of <em>a mile</em>, otherwise <em>seems</em> would not be more "surprising" in (3b) than (3a).</p>
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h3>Sentence processing during reading</h3>
					<h4>Is sentence processing incremental?</h4>
					<p style="text-align: left;" class="fragment">Rayner et al. (2004)</p>
					<p style="text-align: left;" class="fragment">(4a) John used a knife to chop the large <em>carrots</em> for dinner. </p>
					<p style="text-align: left;" class="fragment">(4b) John used a pump to inflate the large <em>carrots</em> for dinner.</p>
					<p style="text-align: left;" class="fragment"><em>Carrots</em> is plausible in (4a) but not (4b).</p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Incrementality in sentence processing</h4>
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h3>Sentence processing during reading</h3>
					<h4>Carry on from yesterday: Is sentence processing incremental?</h4>
					<p style="text-align: left;">Rayner et al. (2004)</p>
					<p style="text-align: left;" class="fragment">Results: Increased reading time on <em>carrots</em> when it's implausible (4b) than plausible (4a). </p>
					<p style="text-align: left;" class="fragment">This effect is as early as the first fixation on the critical word (300ms). </p>
					<p style="text-align: left;" class="fragment">Which suggests that by 300ms, the reader has already begun the process of integrating the word's meaning into the sentence (otherwise plausible vs. implausible wouldn't make a difference).</p>
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h3>Sentence processing during reading</h3>
					<h4>Is sentence processing incremental?</h4>
					<p style="text-align: left;" class="fragment">Syntactic parsing is incremental in reading (Frazier and Rayner (1982)). </p>
					<p style="text-align: left;" class="fragment">Semantic interpretation is incremental in reading (Rayner et al. (2004)). </p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Incrementality in sentence processing</h4>
					<h4>Altmann and Kamide (1999)</h4>
					<p><img src="assets/altmann1999visual.jpg" width="35%"> <img src="assets/altmann1999result.gif" width="45%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Incrementality in sentence processing</h4>
					<p class="fragment" style="text-align: left;">A wide array of information including real-world knowledge as well as linguistic knowledge has been shown to contribute to predictive processing.</p>
					<p class="fragment" style="text-align: left;">including linguistic markers of tense, gender, case, etc.; some phonological patterns; and sentential and/or discourse context. </p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Pragmatic inferencing</h4>
					<h4>Time course of scalar implicature: Huang and Snedeker (2009, 2011)</h4>
					<p style="display: flex; justify-content: center; align-items: center;"><img src="assets/huang2009visual.jpg" width="30%"> <img src="assets/huang2011result.jpg" width="45%"></p>
				</section>
				<section>
					<h3>Speech production in the visual world</h3>
					<h4>Gleitman et al. (2007)</h4>
					<p><img src="assets/gleitman2007visual.jpg" width="70%"></p>
					<p class="fragment" style="text-align: left;">Participants were more likely to mention the cued character first</p>
					<p class="fragment" style="text-align: left;">And subsequently alter their choice of verb ("to chase" vs. "to flee") or their syntactic structure (active vs. passive).</p>
				</section>
				<section data-auto-animate data-auto-animate-id="3">
					<h3>Summary</h3>
					<p class="fragment" style="text-align: left;">The visual world paradigm continues to be a popular tool to investigate spoken language processing.</p>
					<p class="fragment" style="text-align: left;">It has its limitations (e.g. the effect of visual context on language processing).</p>
					<p class="fragment" style="text-align: left;">But it's known for the ability to assess time courses of processing</p>
					<p class="fragment" style="text-align: left;">And its ability to address the interplay of language and vision.</p>
				</section>
				<section data-auto-animate data-auto-animate-id="3">
					<h3>Summary</h3>
					<p><img src="assets/feedback-day-3.png" width="15%"></p>
				</section>
			</div>
            <div class="logo"></div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
                slideNumber: 'c/t',

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
            Reveal.configure({ pdfSeparateFragments: false });
		</script>
	</body>
</html>