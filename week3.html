<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Week 3 Introduction to the Eye-tracking Method and its Application in Language Research</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white-cus.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
                    <h2>Week 3</h2>
                    <h3>Spoken Language Processing in a Visual Context</h3>
					<p><img src="assets/feedback-day-3.png" width="15%"></p>
                </section>
				<section data-auto-animate data-auto-animate-id="1">
                    <h3>The visual world paradigm</h3>
					<p class="fragment" style="text-align: left;">Participants hear/produce an utterance while looking at a visual display</p>
					<p class="fragment" style="text-align: left;">While their eye movements are recorded.</p>
					<p class="fragment" style="text-align: left;">Allows to study real-time language comprehension/production in natural tasks. </p>
                </section>
				<section data-auto-animate data-auto-animate-id="1">
					<h3>The visual world paradigm</h3>
					<h4>The visual display</h4>
					<p class="fragment"><img src="assets/vwe-display.png" width="70%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="1">
					<h3>The visual world paradigm</h3>
					<h4>The task</h4>
					<p class="fragment" style="text-align: left;">Action-based: "Pick up the..."</p>
					<p class="fragment" style="text-align: left;">Non action-based: "Anne went to Starbucks and bought coffee."</p>
				</section>
				<section data-auto-animate data-auto-animate-id="1">
					<h3>The visual world paradigm</h3>
					<h4>The linking hypothesis</h4>
					<p class="fragment" style="text-align: left;">How to link gaze position with language processing?</p>
					<p class="fragment" style="text-align: left;"><b>Activation of linguistic representation &rarr; probability of attention shifted to the corresponding picture &rarr; probability of fixation</b></p>
				</section>
				<section data-auto-animate data-auto-animate-id="1">
					<h3>The visual world paradigm</h3>
					<h4>The linking hypothesis</h4>
					<p><img src="assets/allopenna.png" width="55%"></p>
				</section>
				<section>
					<h3>Word recognition in the visual world</h3>
					<h4>Parallel activation during word recognition: Allopenna et al. (1998)</h4>
					<p><img src="assets/allopenna1998visual.png" width="35%">   <img src="assets/allopenna.png" width="35%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Cooper (1974)</h4>
					<p class="fragment" style="text-align: left;">Rapid eye movements to pictures that were named in stories.</p>
					<p class="fragment" style="text-align: left;">Visual attention highly correlated with spoken sentence processing.</p>
					<p><img src="assets/cooper-1976-visual.png" width="30%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Effect of the visual context: Tanenhause et al. (1995)</h4>
					<p class="fragment" style="text-align: left;">Significantly more early looks to the empty towel in the one-referent than in the two-referent condition. </p>
					<p class="fragment" style="text-align: left;">Listeners rapidly use visual context to disambiguate linguistic input. </p>
					<p><img src="assets/Tanenhaus1995.png" width="35%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Syntactic ambiguities: Snedeker and Trueswell (2004)</h4>
					<p><img src="assets/snedeker2004visual.jpg" width="35%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Syntactic ambiguities: Snedeker and Trueswell (2004)</h4>
					<p><img src="assets/snedeker2004result1.jpg" width="32%"> <img src="assets/snedeker2004result2.jpg" width="32%"> <img src="assets/snedeker2004result3.jpg" width="32%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Incrementality in sentence processing</h4>
					<h4>Altmann and Kamide (1999)</h4>
					<p><img src="assets/altmann1999visual.jpg" width="35%"> <img src="assets/altmann1999result.gif" width="45%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Incrementality in sentence processing</h4>
					<p class="fragment" style="text-align: left;">A wide array of information including real-world knowledge as well as linguistic knowledge has been shown to contribute to predictive processing.</p>
					<p class="fragment" style="text-align: left;">including linguistic markers of tense, gender, case, etc.; some phonological patterns; and sentential and/or discourse context. </p>
				</section>
				<section data-auto-animate data-auto-animate-id="2">
					<h3>Sentence processing in the visual world</h3>
					<h4>Pragmatic inferencing</h4>
					<h4>Time course of scalar implicature: Huang and Snedeker (2009, 2011)</h4>
					<p style="display: flex; justify-content: center; align-items: center;"><img src="assets/huang2009visual.jpg" width="30%"> <img src="assets/huang2011result.jpg" width="45%"></p>
				</section>
				<section>
					<h3>Speech production in the visual world</h3>
					<h4>Griffin and Bock (2000)</h4>
					<p><img src="assets/griffin2000result.png" width="70%"></p>
				</section>
				<section data-auto-animate data-auto-animate-id="3">
					<h3>Summary</h3>
					<p class="fragment" style="text-align: left;">The visual world paradigm continues to be a popular tool to investigate spoken language processing.</p>
					<p class="fragment" style="text-align: left;">It has its limitations (e.g. the effect of visual context on language processing).</p>
					<p class="fragment" style="text-align: left;">But it's known for the ability to assess time courses of processing</p>
					<p class="fragment" style="text-align: left;">And its ability to address the interplay of language and vision.</p>
				</section>
				<section data-auto-animate data-auto-animate-id="3">
					<h3>Summary</h3>
					<p><img src="assets/feedback-day-3.png" width="15%"></p>
				</section>
			</div>
            <div class="logo"></div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
                slideNumber: 'c/t',

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
            Reveal.configure({ pdfSeparateFragments: false });
		</script>
	</body>
</html>